---
title: "W241 Final Project - Tweets are the new folktales"
author: "Arriaga, Hue, Khural, Moran"
date: "August 3, 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Libraries
```{r load libraries}
library(data.table)
library(foreign)
library(tidyverse)
library(dplyr)
library(readr)

# Calculate power
library(effsize)
library(pwr)

# Correlation checks
library(ggpubr)
library(corrplot)
library(PerformanceAnalytics)

# Variance
library(sandwich)

# Reports
library(stargazer)
```

Let's load the data.

```{r load the data}
df<-data.table::fread("../data/processed/tweets_data.csv")
df$truth_f <- as.factor(df$truth)
df$sentiment_f <- as.factor(df$sentiment)

# Robust standard error
rse <- function(model) { 
  sqrt(diag(vcovHC(model)))
  }
```

## Experiment Design

Our main goal is to evaluate if truthfulness is a variable that has a short term impact in memory. We can perform an hypotesis testing comparing all positive tweets to all negative tweers.

Based on the literature, we expect that tweets with fake information are less likely to be remembered. In regards to sentiment, Weizhen Xie found that negative emotions enchance the visual long-term memory. For the power analysis estimation, we will consider the base group to be the treatment with true and negative emotion content. This will be compared with the treatment with the lowest sample size.

## Power analysis

```{r finding base treatment}
# Base group: TN

table1<- table(df$truth_f,df$sentiment_f)
table1

# Base group n=29, the rest all have the same size
# Let's see the least optimistic one: smaller effect

# This line outputs "fact" "fake", meaning fact=0, fake=1
levels(df$truth_f)

# Now for sentiment: negative=0, positive=1
levels(df$sentiment_f)

# The level values are consistent with our literatue (base=TN)
tp<- subset(df, truth_f=="fact" & sentiment_f=="positive")
tn<- subset(df, truth_f=="fact" & sentiment_f=="negative")
fp<- subset(df, truth_f=="fake" & sentiment_f=="positive")
fn<- subset(df, truth_f=="fake" & sentiment_f=="negative")

# Let's compare the coehn's do to see which one has the smaller effect size
coehnd_tn_tp <- cohen.d(tn$total_correct,tp$total_correct)
coehnd_tn_tp
coehnd_tn_fp <- cohen.d(tn$total_correct,fp$total_correct)
coehnd_tn_fp
coehnd_tn_fn <- cohen.d(tn$total_correct,fn$total_correct)
coehnd_tn_fn
```

We selected the comparison between treatment TN and FN because they show a small effect size (d=-0.26).

```{r power analysis}

# We can perform a power test given the effect sizes, samples
pwr.t2n.test(n1 = length(tn$truth) , n2= length(fn$truth), d = coehnd_tn_fn$estimate, sig.level = .95)
```

After performing a power test using the pwr package developed by Champely following the calculation as outlined by Cohen, we found a 96% power (n1=29, n2=26, d=0.25, two sided test). This means that the rest of the treatments, who had a larger effect and same sample size will show a larger power.

# Regression analysis

We performed a regression analysis to evaluate the short-memory retention based on the four treatments.
Three models were proposed to analyze the data: reduced model which compared only the potential outcome (total correct responses) to the covariate of interest tweet truthfulness. A second model, includes the sentiment as a second independent variable and finaly, a third model adds the interaction between both covariates.

y: Total correct responses
F: Content was fake
P: Sentiment was positive

## Model 1: Reduced model

$$y = \beta_0 + \beta_1F + \epsilon_1 \qquad (1)$$

## Model 2: Extended model

$$y = \beta_0 + \beta_1F + \beta_2P + \epsilon_2 \qquad (2)$$

## Model 3: Full model

$$y = \beta_0 + \beta_1F + \beta_2P + \beta_3F\cdot P + \epsilon_3 \qquad  (3)$$

## Correlation check

Before running the analysis we confirmed that our variables are not correlated. We performed a chi-square test to evaluate the categorical covariates Fake and Positive were indepedent.

```{r}
# Let's check if our correlations are linear

cor(as.integer(df$truth_f),as.integer(df$sentiment_f), method = 'pearson')

# There seems to be almost no correlation between the variables.

# Let's plot a correlation matrix
df[, truth_b:= ifelse(truth_f=='fake', yes=1,no=0)]
df[, sentiment_b:= ifelse(sentiment_f=='positive', yes=1,no=0)]

df %>% select(c(truth_b,sentiment_b,total_correct)) %>% chart.Correlation()

```

Now we compute the three proposed models.

```{r logit model 1}
model_1 <- df[, lm(total_correct~truth_f)]
summary(model_1, vcov=vcovHC(model_1))
```

Looking at the linear regression, we didn't find a correlation between fake tweets and memory retention. We fail to reject the null hypothesis in our reduced model.

```{r logit model 2}
model_2 <- df[, lm(total_correct~truth_f+sentiment_f)]
summary(model_2, vcov=vcovHC(model_2))
```

When running the extended model, we found that the fake covariate coefficient is reduced to 0.39 ($\delta\beta_1=-0.03$) and the standard error was slightly reduced ($\delta\sigma_f=0.01$). We found that sentiment had a significant positive impact in short-term memory retention by 0.73 (p-val=0.007, CI=95%), which is closer to remembering the content of one more tweet.

```{r logit model 3}
model_3 <- df[, lm(total_correct~truth_f+sentiment_f + truth_f:sentiment_f)]
summary(model_3, vcov=vcovHC(model_3))
```

After running the full model, we found none of the covariates had a significant result.

Let's organize the results in an organized comparison.

```{r stargazer three models}
# stargazer of all models
stargazer(model_1, model_2, model_3,
  # type="text",
  se = list(rse(model_1),rse(model_2), rse(model_3)),
column.labels = c("F","F+P","F+P+F*P"),
dep.var.labels   = "Correct responses",
covariate.labels = c("Fake information", "Positive sentiment", "Fake:Positive"),
# add.lines = list(c("optimized model", "Yes", "Yes","Yes")),
header=FALSE, type='text',
title = "Regression results"
)
```

We found that while fake information doesn't seem to correlate with short-term memory retention, the positive sentiment of tweets does have a significant positive impact in the short-memory retention when explosed to tweets. This is an interesting finding because it's opposite to what the literature suggested.

## Individual tweet analysis

Will this results be consistent with individual tweets? We ran a linear regression and clustered it by each tweet to see the error depended on specific tweets.

```{r}
model_q_one <- df[ ,lm(bin_georgians~ truth_f+sentiment_f+truth_f:sentiment_f)]
model_q_two <- df[ ,lm(bin_energy~ truth_f+sentiment_f+truth_f:sentiment_f)]
model_q_three <- df[ ,lm(bin_soccer~ truth_f+sentiment_f+truth_f:sentiment_f)]
model_q_four <- df[ ,lm(bin_pollution~ truth_f+sentiment_f+truth_f:sentiment_f)]
model_q_five <- df[ ,lm(bin_fauci~ truth_f+sentiment_f+truth_f:sentiment_f)]
model_q_six <- df[ ,lm(bin_election~ truth_f+sentiment_f+truth_f:sentiment_f)]

# stargazer by question
stargazer(model_q_one, model_q_two, model_q_three,
          model_q_four,model_q_five,model_q_six,
  # type="text",
  se = list(rse(model_q_one),rse(model_q_two), rse(model_q_three),
            rse(model_q_four),rse(model_q_five),rse(model_q_six)),
column.labels = c("Georgians","Energy","Soccer","Pollution","Fauci","Election"),
dep.var.labels =  c("","","","","",""),
covariate.labels = c("Fake information", "Positive sentiment", "Fake:Positive"),
# add.lines = list(c("optimized model", "Yes", "Yes","Yes")),
header=FALSE, type='text',
title = "Regression results"
)
```

After analyzing the responses by question type in the full model, we found that there is a short-memory retention effect in the principal covariate truthfulness, some cases the sentiment has an effect and also the interaction between fake and positive. This suggests that there could be a relationship between the topic (ommited variable bias) and the short-memory retention.

Entertainment, politics and health topics seem to have a stronger effect than environmental related ones such as pollution or energy. Only the question about statements about georgians during the elections seemed to be impacted positively by the sentiment. Also, we found a strong interaction effect between the Fauci, which mentioned the Covi-19 pandemic, which is a very controversial topic. The fauci tweet had the largest F-statistic from all the cases (F=9.2), it showed a more dramatic response with less variance ($R_{adj}=0.19$).



## Descriptive statistics

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

## Randomization check



